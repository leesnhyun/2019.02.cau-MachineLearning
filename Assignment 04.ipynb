{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\title{Assignment 04}\n",
    "\\author{20142921 SengHyun Lee}\n",
    "\\date{2019.10.30}\n",
    "\\maketitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification based on 3 layers neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First layer\n",
    "\n",
    "$Z^{[1]} = W^{[1]} X + b^{[1]}$ : $X$ denotes the input data\n",
    "$A^{[1]} = g^{[1]}(Z^{[1]})$ : $g^{[1]}$ is the activation function at the first layer\n",
    "\n",
    "#### Second layer\n",
    "\n",
    "$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$\n",
    "\n",
    "$A^{[2]} = g^{[2]}(Z^{[2]})$ : $g^{[2]}$ is the activation function at the second layer\n",
    "\n",
    "#### Third layer\n",
    "\n",
    "$Z^{[3]} = W^{[3]} A^{[2]} + b^{[3]}$\n",
    "\n",
    "$A^{[3]} = g^{[3]}(Z^{[3]})$ : $g^{[3]}$ is the activation function at the third (output) layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library & plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_plot(g1, g2, title, color, label, legend):\n",
    "    plt.title(title)\n",
    "    plt.plot(np.arange(1, len(g1) + 1), g1, color=color[0], alpha=0.5, label=label[0])\n",
    "    plt.plot(np.arange(1, len(g2) + 1), g2, color=color[1], alpha=0.5, label=label[1])\n",
    "    plt.legend(loc=legend)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def output_frame_plot(tloss, vloss, tacc, vacc, title):\n",
    "    print(\" << %s >>\" % title)\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"           |   loss   |  accuracy  |\")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"training   |   %.2f   |    %.2f    |\" % (tloss, tacc))\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"validation |   %.2f   |    %.2f    |\" % (vloss, vacc))\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 100\n",
    "IMAGE_HEIGHT = 100\n",
    "IMAGE_CHANNEL = 1\n",
    "DIMENSION = IMAGE_CHANNEL * IMAGE_HEIGHT * IMAGE_WIDTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train & validation datasets (preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batch size = 3\n",
    "* number of workers = 1 (main process + worker1)\n",
    "* number of epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(batch_size=3, num_workers=1):\n",
    "    transform = transforms.Compose([  # transforms.Resize((256,256)),\n",
    "        transforms.Grayscale(),\n",
    "        # the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "        transforms.ToTensor(), ])\n",
    "\n",
    "    # train_data_path = 'relative path of training data set'\n",
    "    train_data_path = './horse-or-human/train'\n",
    "    trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "    # change the valuse of batch_size, num_workers for your program\n",
    "    # if shuffle=True, the data reshuffled at every epoch\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    validation_data_path = './horse-or-human/validation'\n",
    "    valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "    # change the valuse of batch_size, num_workers for your program\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    train_data = np.empty((DIMENSION, 0))\n",
    "    validation_data = np.empty((DIMENSION, 0))\n",
    "\n",
    "    train_label = np.array([])\n",
    "    validation_label = np.array([])\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "\n",
    "        # [batch_size, 1, height, width] => [ width * height * channel, batch_size ]\n",
    "        x = np.array(inputs).transpose((2, 3, 0, 1)).reshape((DIMENSION, len(labels)))\n",
    "        train_data = np.concatenate((train_data, x), axis=1)\n",
    "        train_label = np.concatenate((train_label, np.array(labels)))\n",
    "\n",
    "    # load validation images of the batch size for every iteration\n",
    "    for i, data in enumerate(valloader):\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # [batch_size, 1, height, width] => [ width * height * channel, batch_size ]\n",
    "        x = np.array(inputs).transpose((2, 3, 0, 1)).reshape((DIMENSION, len(labels)))\n",
    "        validation_data = np.concatenate((validation_data, x), axis=1)\n",
    "        validation_label = np.concatenate((validation_label, np.array(labels)))\n",
    "\n",
    "    return train_data, validation_data, train_label, validation_label\n",
    "\n",
    "\n",
    "t_data, v_data, t_label, v_label = pre_process(batch_size=3, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implements of 3 layers neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### sigmoid\n",
    "$g(z) = \\frac{1}{1 + \\exp^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### tanh\n",
    "$g(z) = \\frac{\\exp^{z} - \\exp^{-z}}{\\exp^{z} + \\exp^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ReLU\n",
    "$g(z) = \\max(0, z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Leaky ReLU\n",
    "$g(z) = \\max(0.01z, z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (Epsilon) $\\mathcal{e} = 10^{-6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input layer\n",
    "* num of features = $10000$ (100 * 100 image)\n",
    "* num of samples = $1027$ (number of training image samples)\n",
    "\n",
    "#### hidden layer 1\n",
    "* num of features (nodes) = $150$\n",
    "\n",
    "#### hidden layer 2\n",
    "* num of features (nodes) = $50$\n",
    "\n",
    "#### output layer\n",
    "* num of features (nodes) = $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classifier\n",
    "def binary_classify(train_data, validation_data,\n",
    "                    train_label, validation_label,\n",
    "                    gn_act, gn_d_act, learning_rate=0.0002):\n",
    "\n",
    "    num_of_layers = 3\n",
    "    n1, n2 = 150, 50\n",
    "    learning_rate = learning_rate\n",
    "    epsilon = 10e-6\n",
    "\n",
    "    # INITIALIZE u v z\n",
    "    u = np.random.randn(DIMENSION, n1) * np.sqrt(2 / (DIMENSION + n1))\n",
    "    v = np.random.randn(n1, n2) * np.sqrt(2 / (n1 + n2))\n",
    "    w = np.random.randn(n2, 1) * np.sqrt(2 / (n2 + 1))\n",
    "\n",
    "    # INITIALIZE bias\n",
    "    b1 = np.random.randn(n1, 1)\n",
    "    b2 = np.random.randn(n2, 1)\n",
    "    b3 = np.random.randn(1, 1)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    def safe_ln(x, minval=10e-20):\n",
    "        return np.log(x.clip(min=minval))\n",
    "\n",
    "    def cross_entropy(prob, ans):\n",
    "        return -(np.nan_to_num(ans * np.log(prob)) + np.nan_to_num((1 - ans) * np.log(1-prob)))\n",
    "        # return -(ans * safe_ln(prob) + (1 - ans) * safe_ln(1-prob))\n",
    "\n",
    "    def loss(prob, ans):\n",
    "        return (1 / len(ans)) * np.nan_to_num(np.sum(cross_entropy(prob, ans)))\n",
    "\n",
    "    def accuracy(prob, ans):\n",
    "        arr = np.array(list(map(lambda x: 1 if x > 0.5 else 0, prob.flatten())))\n",
    "        arr = list(filter(lambda x: x == 0, arr - ans))\n",
    "        return len(arr) / len(ans)\n",
    "\n",
    "    def iterate():\n",
    "        p_train_loss = 0\n",
    "        nonlocal u, v, w, b1, b2, b3\n",
    "        nonlocal train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # forward propagation #\n",
    "            act = gn_act()\n",
    "            next(act)\n",
    "            z1 = np.dot(u.T, train_data) + b1\n",
    "            a1 = act.send(z1)\n",
    "\n",
    "            z2 = np.dot(v.T, a1) + b2\n",
    "            a2 = act.send(z2)\n",
    "\n",
    "            z3 = np.dot(w.T, a2) + b3\n",
    "            a3 = act.send(z3)\n",
    "\n",
    "            act = gn_act()\n",
    "            next(act)\n",
    "            vz = np.dot(u.T, validation_data) + b1\n",
    "            vz = np.dot(v.T, act.send(vz)) + b2\n",
    "            vz = np.dot(w.T, act.send(vz)) + b3\n",
    "            ####\n",
    "\n",
    "            # back propagation #\n",
    "            d_act = gn_d_act()\n",
    "            next(d_act)\n",
    "\n",
    "            cw = (a3 - train_label)\n",
    "            dw = np.dot(cw, a2.T) / z3.shape[1]\n",
    "\n",
    "            cv = np.dot(w, cw) * d_act.send(z2)\n",
    "            dv = np.dot(cv, a1.T) / z3.shape[1]\n",
    "\n",
    "            cu = np.dot(v, cv) * d_act.send(z1)\n",
    "            du = np.dot(cu, train_data.T) / z3.shape[1]\n",
    "\n",
    "            b3 = b3 - (learning_rate * (np.sum(cw, axis=1, keepdims=True) / z3.shape[1]))\n",
    "            b2 = b2 - (learning_rate * (np.sum(cv, axis=1, keepdims=True) / z3.shape[1]))\n",
    "            b1 = b1 - (learning_rate * (np.sum(cu, axis=1, keepdims=True) / z3.shape[1]))\n",
    "\n",
    "            # gradient descent #\n",
    "            w = w - (learning_rate * dw).T\n",
    "            v = v - (learning_rate * dv).T\n",
    "            u = u - (learning_rate * du).T\n",
    "            ####\n",
    "\n",
    "            # get losses\n",
    "            t_hat, v_hat = a3, act.send(vz)\n",
    "\n",
    "            n_train_loss = loss(t_hat, train_label)\n",
    "            n_test_loss = loss(v_hat, validation_label)\n",
    "\n",
    "            # get accuracies\n",
    "            n_train_acc = accuracy(t_hat, train_label)\n",
    "            n_test_acc = accuracy(v_hat, validation_label)\n",
    "\n",
    "            train_losses.append(n_train_loss)\n",
    "            test_losses.append(n_test_loss)\n",
    "            train_accuracies.append(n_train_acc)\n",
    "            test_accuracies.append(n_test_acc)\n",
    "\n",
    "            if abs(p_train_loss - n_train_loss) < epsilon:\n",
    "                break\n",
    "            else:\n",
    "                # print('t loss: %s, v loss: %s' % (n_train_loss, n_test_loss))\n",
    "                p_train_loss = n_train_loss\n",
    "                continue\n",
    "\n",
    "    iterate()\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(case, title):\n",
    "\n",
    "    leaky_alpha = 0.01\n",
    "\n",
    "    t_data, v_data, t_label, v_label = pre_process(batch_size=3, num_workers=1)\n",
    "    train_loss, test_loss, train_acc, test_acc = [], [], [], []\n",
    "\n",
    "    # functions\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def d_sigmoid(z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "    def tanh(z):\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "    def d_tanh(z):\n",
    "        return 1 - (tanh(z) ** 2)\n",
    "\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def d_relu(z):\n",
    "        return np.where(z <= 0, 0, 1)\n",
    "\n",
    "    def leaky_relu(z):\n",
    "        return np.maximum(leaky_alpha * z, z)\n",
    "\n",
    "    def d_leaky_relu(z):\n",
    "        return np.where(z <= 0, leaky_alpha, 1)\n",
    "\n",
    "    # case-studies\n",
    "    def case1(learning_rate):\n",
    "        def act():\n",
    "            z = yield\n",
    "            z = yield sigmoid(z)\n",
    "            z = yield sigmoid(z)\n",
    "            z = yield sigmoid(z)\n",
    "\n",
    "        def d_act():\n",
    "            z = yield\n",
    "            z = yield d_sigmoid(z)\n",
    "            z = yield d_sigmoid(z)\n",
    "\n",
    "        classify(gn=act, dgn=d_act, learning_rate=learning_rate)\n",
    "        plot()\n",
    "\n",
    "    def case2(learning_rate):\n",
    "        def act():\n",
    "            z = yield\n",
    "            z = yield tanh(z)\n",
    "            z = yield tanh(z)\n",
    "            z = yield sigmoid(z)\n",
    "\n",
    "        def d_act():\n",
    "            z = yield\n",
    "            z = yield d_tanh(z)\n",
    "            z = yield d_tanh(z)\n",
    "\n",
    "        classify(gn=act, dgn=d_act, learning_rate=learning_rate)\n",
    "        plot()\n",
    "\n",
    "    def case3(learning_rate):\n",
    "        def act():\n",
    "            z = yield\n",
    "            z = yield relu(z)\n",
    "            z = yield relu(z)\n",
    "            z = yield sigmoid(z)\n",
    "\n",
    "        def d_act():\n",
    "            z = yield\n",
    "            z = yield d_relu(z)\n",
    "            z = yield d_relu(z)\n",
    "\n",
    "        classify(gn=act, dgn=d_act, learning_rate=learning_rate)\n",
    "        plot()\n",
    "\n",
    "    def case4(learning_rate):\n",
    "        def act():\n",
    "            z = yield\n",
    "            z = yield leaky_relu(z)\n",
    "            z = yield leaky_relu(z)\n",
    "            z = yield sigmoid(z)\n",
    "\n",
    "        def d_act():\n",
    "            z = yield\n",
    "            z = yield d_leaky_relu(z)\n",
    "            z = yield d_leaky_relu(z)\n",
    "\n",
    "        classify(gn=act, dgn=d_act, learning_rate=learning_rate)\n",
    "        plot()\n",
    "\n",
    "    def classify(gn, dgn, learning_rate):\n",
    "        nonlocal train_loss, test_loss, train_acc, test_acc\n",
    "\n",
    "        train_loss, test_loss, train_acc, test_acc = binary_classify(\n",
    "            t_data, v_data,\n",
    "            t_label, v_label,\n",
    "            gn, dgn, learning_rate\n",
    "        )\n",
    "\n",
    "    def plot():\n",
    "        output_plot(train_loss, test_loss,\n",
    "                    title=\"Loss (ENERGY) :: \" + title, color=('blue', 'red'),\n",
    "                    label=('train loss', 'validation loss'), legend='upper right')\n",
    "\n",
    "        output_plot(train_acc, test_acc,\n",
    "                    title=\"Accuracy :: \" + title, color=('blue', 'red'),\n",
    "                    label=('train accuracy', 'validation accuracy'), legend='lower right')\n",
    "\n",
    "        output_frame_plot(train_loss[-1], test_loss[-1], train_acc[-1], test_acc[-1], title=title)\n",
    "\n",
    "    if case == 1:\n",
    "        case1(learning_rate=0.01)\n",
    "    elif case == 2:\n",
    "        case2(learning_rate=0.0005)\n",
    "    elif case == 3:\n",
    "        case3(learning_rate=0.0005)\n",
    "    elif case == 4:\n",
    "        case4(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $g^{[1]}, g^{[2]}, g^{[3]}$ are Sigmoid\n",
    "* learning rate = $0.005$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(case=1, title=\"sigmoid sigmoid sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $g^{[1]}, g^{[2]}$ are tanh and $g^{[3]}$ is Sigmoid\n",
    "* learning rate = $0.0005$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(case=2, title=\"tanh tanh sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $g^{[1]}, g^{[2]}$ are ReLU and $g^{[3]}$ is Sigmoid\n",
    "* learning rate = $0.0005$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(case=3, title=\"ReLU ReLU sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $g^{[1]}, g^{[2]}$ are Leaky ReLU ($\\alpha=0.01)$ and $g^{[3]}$ is Sigmoid\n",
    "* learning rate = $0.0005$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(case=4, title=\"Leaky-ReLU Leaky-ReLU sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
