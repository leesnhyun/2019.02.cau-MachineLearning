{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\title{Assignment 03}\n",
    "\\author{20142921 SengHyun Lee}\n",
    "\\date{2019.10.10}\n",
    "\\maketitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification based on 3 layers neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library & plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_plot(g1, g2, title, color, label, legend):\n",
    "    plt.title(title)\n",
    "    plt.plot(np.arange(1, len(g1) + 1), g1, color=color[0], alpha=0.5, label=label[0])\n",
    "    plt.plot(np.arange(1, len(g2) + 1), g2, color=color[1], alpha=0.5, label=label[1])\n",
    "    plt.legend(loc=legend)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def output_frame_plot(tloss, vloss, tacc, vacc):\n",
    "    print(\"           |   loss   |  accuracy  |\")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"training   |   %.2f   |    %.2f    |\" % (tloss, tacc))\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"validation |   %.2f   |    %.2f    |\" % (vloss, vacc))\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 100\n",
    "IMAGE_HEIGHT = 100\n",
    "IMAGE_CHANNEL = 1\n",
    "DIMENSION = IMAGE_CHANNEL * IMAGE_HEIGHT * IMAGE_WIDTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train & validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batch size = 3\n",
    "* number of workers = 1 (main process + worker1)\n",
    "* number of epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(batch_size=3, num_workers=1):\n",
    "    transform = transforms.Compose([  # transforms.Resize((256,256)),\n",
    "        transforms.Grayscale(),\n",
    "        # the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "        transforms.ToTensor(), ])\n",
    "\n",
    "    # train_data_path = 'relative path of training data set'\n",
    "    train_data_path = './horse-or-human/train'\n",
    "    trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "    # change the valuse of batch_size, num_workers for your program\n",
    "    # if shuffle=True, the data reshuffled at every epoch\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    validation_data_path = './horse-or-human/validation'\n",
    "    valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "    # change the valuse of batch_size, num_workers for your program\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    train_data = np.empty((DIMENSION, 0))\n",
    "    validation_data = np.empty((DIMENSION, 0))\n",
    "\n",
    "    train_label = np.array([])\n",
    "    validation_label = np.array([])\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "\n",
    "        # [batch_size, 1, height, width] => [ width * height * channel, batch_size ]\n",
    "        x = np.array(inputs).transpose((2, 3, 0, 1)).reshape((DIMENSION, len(labels)))\n",
    "        train_data = np.concatenate((train_data, x), axis=1)\n",
    "        train_label = np.concatenate((train_label, np.array(labels)))\n",
    "\n",
    "    # load validation images of the batch size for every iteration\n",
    "    for i, data in enumerate(valloader):\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # [batch_size, 1, height, width] => [ width * height * channel, batch_size ]\n",
    "        x = np.array(inputs).transpose((2, 3, 0, 1)).reshape((DIMENSION, len(labels)))\n",
    "        validation_data = np.concatenate((validation_data, x), axis=1)\n",
    "        validation_label = np.concatenate((validation_label, np.array(labels)))\n",
    "\n",
    "    return train_data, validation_data, train_label, validation_label\n",
    "\n",
    "\n",
    "t_data, v_data, t_label, v_label = pre_process(batch_size=3, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implements of 3 layers neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (Learning Rate) $\\alpha = 0.055$\n",
    "* (Epsilon) $\\mathcal{e} = 10^{-6}$\n",
    "\n",
    "* $(x_i, y_i)$ denotes a pair of a training example and $i = 1, 2, \\cdots, n$\n",
    "* $p_i = \\sigma(u^T x_i + a)$ (hidden layer)\n",
    "* $q_i = \\sigma(v^T p_i + b)$ (hidden layer)\n",
    "* $\\hat{y}_i = \\sigma(w^T q_i + c)$ (output layer)\n",
    "\n",
    "The logistic function $\\sigma$ is defined by $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$\n",
    "The loss function is defined by $\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n f_i(u, v, w, a, b, c)$\n",
    "\n",
    "* $f_i(u, v, w, a, b, c) = - y_i \\log \\hat{y}_i - (1 - y_i) \\log (1 - \\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![architecture](neural_net2.jpeg)\n",
    "\n",
    "#### Input layer\n",
    "* num of features = $10000$ (100 * 100 image)\n",
    "* num of samples = $1027$ (number of training image samples)\n",
    "\n",
    "#### hidden layer 1\n",
    "* num of features (nodes) = $150$\n",
    "* initial value of parameter : $u \\thicksim N(1,0)$\n",
    "* activation function : sigmoid\n",
    "\n",
    "#### hidden layer 2\n",
    "* num of features (nodes) = $50$\n",
    "* initial value of parameter : $v \\thicksim N(1,0)$\n",
    "* activation function : sigmoid\n",
    "\n",
    "#### output layer\n",
    "* num of features (nodes) = $1$\n",
    "* initial value of parameter : $w \\thicksim N(1,0)$\n",
    "* activation function : sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in log\n",
      "/home/sh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "def binary_classify(train_data, validation_data, train_label, validation_label):\n",
    "    num_of_layers = 3\n",
    "    num_of_nodes = 500\n",
    "    learning_rate = 0.04\n",
    "    epsilon = 10e-6\n",
    "\n",
    "    # INITIALIZE u v z\n",
    "    u = np.random.randn(DIMENSION+1, num_of_nodes*4)\n",
    "    v = np.random.randn(num_of_nodes*4, num_of_nodes)\n",
    "    w = np.random.randn(num_of_nodes, 1)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def d_sigmoid(z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "    def cross_entropy(prob, ans):\n",
    "        return -(np.nan_to_num(ans * np.log(prob)) + np.nan_to_num((1 - ans) * np.log(1 - prob)))\n",
    "\n",
    "    def loss(prob, ans):\n",
    "        return (1 / len(ans)) * np.nan_to_num(np.sum(cross_entropy(prob, ans)))\n",
    "\n",
    "    def accuracy(prob, ans):\n",
    "        arr = np.array(list(map(lambda x: 1 if x > 0.5 else 0, prob.flatten())))\n",
    "        arr = list(filter(lambda x: x == 0, arr - ans))\n",
    "        return len(arr) / len(ans)\n",
    "\n",
    "    def du(x, a, b, c, v, w, cached):\n",
    "        return (1/c.shape[1]) * np.dot(np.dot(v, np.dot(w, cached) * d_sigmoid(b)) * d_sigmoid(a), x.T)\n",
    "\n",
    "    def dv(a, b, c, w, cached):\n",
    "        return (1/c.shape[1]) * np.dot(np.dot(w, cached) * d_sigmoid(b), sigmoid(a).T)\n",
    "\n",
    "    def dw(b, c, cached):\n",
    "        return (1/c.shape[1]) * np.dot(cached, sigmoid(b).T)\n",
    "\n",
    "    def iterate():\n",
    "        p_train_loss = 0\n",
    "        nonlocal u, v, w\n",
    "        nonlocal train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "        train_data_with_bias = np.concatenate((train_data, np.ones((1, train_data.shape[1]))))\n",
    "        validation_data_with_bias = np.concatenate((validation_data, np.ones((1, validation_data.shape[1]))))\n",
    "\n",
    "        while True:\n",
    "            # forward propagation\n",
    "            a = np.dot(u.T, train_data_with_bias)\n",
    "            b = np.dot(v.T, sigmoid(a))\n",
    "            c = np.dot(w.T, sigmoid(b))\n",
    "\n",
    "            vz = np.dot(u.T, validation_data_with_bias)\n",
    "            vz = np.dot(v.T, sigmoid(vz))\n",
    "            vz = np.dot(w.T, sigmoid(vz))\n",
    "\n",
    "            # backward propagation\n",
    "            cached = (sigmoid(c) - train_label)\n",
    "            w = w - (learning_rate * dw(b, c, cached)).T\n",
    "            v = v - (learning_rate * dv(a, b, c, w, cached)).T\n",
    "            u = u - (learning_rate * du(train_data_with_bias, a, b, c, v, w, cached)).T\n",
    "\n",
    "            n_train_loss = loss(sigmoid(c), train_label)\n",
    "            n_test_loss = loss(sigmoid(vz), validation_label)\n",
    "\n",
    "            n_train_acc = accuracy(sigmoid(c), train_label)\n",
    "            n_test_acc = accuracy(sigmoid(vz), validation_label)\n",
    "\n",
    "            # gathering results\n",
    "            train_losses.append(n_train_loss)\n",
    "            test_losses.append(n_test_loss)\n",
    "            train_accuracies.append(n_train_acc)\n",
    "            test_accuracies.append(n_test_acc)\n",
    "\n",
    "            if abs(p_train_loss - n_train_loss) < epsilon:\n",
    "                break\n",
    "            else:\n",
    "                p_train_loss = n_train_loss\n",
    "                continue\n",
    "\n",
    "    iterate()\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "\n",
    "t_data, v_data, t_label, v_label = pre_process(batch_size=3, num_workers=1)\n",
    "\n",
    "train_loss, test_loss, train_acc, test_acc = binary_classify(t_data, v_data, t_label, v_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_plot(train_loss, test_loss,\n",
    "            title=\"Loss\", color=('blue', 'red'),\n",
    "            label=('train loss', 'validation loss'), legend='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_plot(train_acc, test_acc,\n",
    "            title=\"Accuracy\", color=('blue', 'red'),\n",
    "            label=('train accuracy', 'validation accuracy'), legend='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frame_plot(train_loss[-1], test_loss[-1], train_acc[-1], test_acc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
